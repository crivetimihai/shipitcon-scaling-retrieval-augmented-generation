Large Language Models and their Limitations
============================================

GenAI and Large Language Models Explained
------------------------------------------

### Think of LLMs like mathematical functions, or your phone's autocomple

`f(x) = x'`

- Where the input (x) and the output (x') are strings

### A more accurate representation 

`f(training_data, model_parameters, input_string) = output_string`

- training_data represents the data it was trained.
- model_parameters represent things like "temperature"
- input_string is the combination of prompt and context you give to the model. Ex: "What is Kubernetes" or "Summarize the following document: ${DOCUMENT}"
- the 'prompt' is usually an optional instruction like "summarize", "extract", "translate", "classify" etc. but more complex prompts are usually used. "Be a helpful assistant that responds to my question.. etc."
- The function can process a maximum of TOKEN_LIMIT (total input and output), usually ~4096 tokens (~3000 words in English, fewer in say.. Japanese).

What Large Language Models DON'T DO
------------------------------------

### Learn

A model will not 'learn' from interactions (unless specifically trained/fine-tuned).

### Remember

A model doesn't remember previous prompts. In fact, it's all done with prompt trickery: previous prompts are injected.
The API does a LOT of of filtering and heavy lifting!

### Reason

Think of LLMs like your phone's autocomplete, it doesn't reason, or do math.

### Use your data

LLMs don't provide responses based on YOUR data (databases or files), unless it's include in the training dataset, or the prompt (ex: RAG).

### Use the Internet

- A LLM doesn't have the capacity to 'search the internet', or make API calls.
- In fact, a mdoel does not perform *any* activity other than converting one string of text into another string of text.
- Any 3rd party data not in the model will need to be injected into prompts (RAG)

GenAI Use Cases
--------------

![Use Cases](img/00-use-cases-tb.png)


Think of adding this to your architecture
------------------------------------------

![Adding an LLM to your architecture often feels like this](img/modem.png)

In fact, even a 9600 baud modem is much faster. Think teletype!
---------------------------------------------------------------

### LLMs are *really slow*

- A  WPM = ((BPS / 10) / 5) * 60, a 9600 baud modem will generate 11520 words / minute.
- At an average 30 tokens / second (20 words) for LLAMA-70B, you're getting 1200 words / minute!
- This is slower than a punch card reader :-)

### LLMs are also expensive to run

- Running your own LLAMA2 70B might cost as much as 20K / month if you're using a dedicated GPU instance!

Model Limitations in Practice
---------------------------

### Latency and Bandwidth: Tokens per second

- Large models (70B) such as LLAMA2 can be painfully slow
- Smaller models (20B, 13B, 7B) are faster, and can perform inference on a cheaper GPU (less VRAM)
- Even so, models will perform at anywhere between 10 - 100 tokens / second.
- In practice, that's painfull slow

### Token Limits

- Models have a token limit as well
- Usually 4096 tokens (roughly ~3000 words) of total input and output


::: notes
https://www.anyscale.com/blog/num-every-llm-developer-should-know
::: 


Getting LLMs to work with our data
----------------------------------

### Training

- Very Expensive, takes a long time

### Fine Tuning

- Expensive, takes considerable time as well, but achievable

### Retrieval Augmented Generation

- Insert your data into prompts every time
- Cheap, and can work with vast amounts of data
- While LLMs are SLOW, Vector Databases are FAST!
- Can help overcome model limitations (such as token limits) - as you're only feeding 'top search results' to the LLM, instead of whole documents.

Retrieval Augmented Generation or Conversation with your Documents
===================================================================

RAG Explained
--------------

![RAG Explained](img/01-rag-explained-datail.png)


Loading Documents
-----------------

![Loading Documents](img/02-loading-documents.png)


Scaling factor for RAG
----------------------

- Vector Database: consider sharding and High Availability
- Fine Tuning: collecting data to be used for fine tuning
- Governance and Model Benchmarking: how are you testing your model performance over time, with different prompts, one-shot, and various parameters
- Chain of Reasoning and Agents
- Caching embeddings and responses
- Personalization and Conversational Memory Database
- Streaming Responses and optimizing performance. A fine tuned 13B model may perform better than a poor 70B one!
- Calling 3rd party functions or APIs for reasoning or other type of data (ex: LLMs are terrible at reasoning and prediction, consider calling other models)
- Fallback techniques: fallback to a different model, or default answers
- API scaling techniques, rate limiting, etc.
- Async, streaming and parallelization, multiprocessing, GPU acceleration (including embeddings), generating your API using OpenAPI, etc.

Contact
-------

- This Talk in markdown format: https://github.com/crivetimihai/shipitcon-scaling-retrieval-augmented-generation
- https://twitter.com/CrivetiMihai
- https://youtube.com/CrivetiMihai - more LLM videos to follow
- https://www.linkedin.com/in/crivetimihai/
