Large Language Models and their Limitations
============================================

GenAI and Large Language Models Explained
------------------------------------------

### Think of LLMs like mathematical functions, or your phone's autocomple

- f(x) = x'
- Where the input (x) is a string
- And the output x' is also a string.

### A more accurate representation 

f(training_data, model_parameters, input_string) = output_string

- training_data represents the data it was trained.
- model_parameters represent things like "temperature"
- input_string is the combination of prompt and context you give to the model. Ex: "What is Kubernetes" or "Summarize the following document: ${DOCUMENT}"
- the 'prompt' is usually an optional instruction like "summarize", "extract", "translate", "classify" etc. but more complex prompts are usually used. "Be a helpful assistant that responds to my question.. etc."
- The function can process a maximum of TOKEN_LIMIT (total input and output), usually ~4096 tokens (~3000 words in English, fewer in say.. Japanese).

What Large Language Models don't do
------------------------------------

### LLMS will NOT

- Learn: a model will not 'learn' from previous experiences, unless it's retrained or fine-tuned on the data.
- Remember: a model doesn't remember previous prompts. In fact, it's all done with prompt trickery: previous prompts are injected
- Reason: think of LLMs like your phone's autocomplete, it doesn't reason, or do math.
- Provide responses based on YOUR data, unless it's include in the training dataset, or the prompt (ex: RAG)


GenAI Use Cases
--------------

![Use Cases](img/00-use-cases-tb.png)


Think of adding this to your architecture
------------------------------------------

![Adding an LLM to your architecture often feels like this](img/modem.png)

In fact, even a 9600 baud modem is much faster. Think teletype!
---------------------------------------------------------------

### LLMs are *really slow*

- A  WPM = ((BPS / 10) / 5) * 60, a 9600 baud modem will generate 11520 words / minute.
- At an average 30 tokens / second (20 words) for LLAMA-70B, you're getting 1200 words / minute!
- This is slower than a punch card reader :-)


Model Limitations in Practice
---------------------------

### Latency and Bandwidth: Tokens per second

- Large models (70B) such as LLAMA2 can be painfully slow
- Smaller models (20B, 13B, 7B) are faster, and can perform inference on a cheaper GPU (less VRAM)
- Even so, models will perform at anywhere between 10 - 100 tokens / second.
- In practice, that's painfull slow

### Token Limits

- Models have a token limit as well
- Usually 4096 tokens (roughly ~3000 words) of total input and output


::: notes
https://www.anyscale.com/blog/num-every-llm-developer-should-know
::: 

Retrieval Augmented Generation or Conversation with your Documents
===================================================================

RAG Explained
--------------

![RAG Explained](img/01-rag-explained-datail.png)


Loading Documents
-----------------

![Loading Documents](img/02-loading-documents.png)


Contact
-------

- https://twitter.com/CrivetiMihai
- https://youtube.com/CrivetiMihai - more LLM videos to follow
- https://www.linkedin.com/in/crivetimihai/
